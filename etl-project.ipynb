{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import os\n",
    "import sqlalchemy\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a connection to the database\n",
    "engine = create_engine(f'postgresql://postgres:postgres@localhost:5432/HigherEducation')\n",
    "con = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### college_names table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### College Scorecard Data Dictionary\n",
    "\n",
    "HIGHDEG = {\n",
    "\t0:\t'Non-degree-granting',\n",
    "\t1:\t'Certificate degree',\n",
    "\t2:\t'Associate degree',\n",
    "\t3:\t'Bachelors degree',\n",
    "\t4:\t'Graduate degree'\n",
    "}\n",
    "\n",
    "ICLEVEL = {\n",
    "\t1:\t'4-year',\n",
    "\t2:\t'2-year',\n",
    "\t3:\t'Less-than-2-year'\n",
    "}\n",
    "\n",
    "REGION = {\n",
    "\t0:\t'U.S. Service Schools',\n",
    "\t1:\t'New England (CT, ME, MA, NH, RI, VT)',\n",
    "\t2:\t'Mid East (DE, DC, MD, NJ, NY, PA)',\n",
    "\t3:\t'Great Lakes (IL, IN, MI, OH, WI)',\n",
    "\t4:\t'Plains (IA, KS, MN, MO, NE, ND, SD)',\n",
    "\t5:\t'Southeast (AL, AR, FL, GA, KY, LA, MS, NC, SC, TN, VA, WV)',\n",
    "\t6:\t'Southwest (AZ, NM, OK, TX)',\n",
    "\t7:\t'Rocky Mountains (CO, ID, MT, UT, WY)',\n",
    "\t8:\t'Far West (AK, CA, HI, NV, OR, WA)',\n",
    "\t9:\t'Outlying Areas (AS, FM, GU, MH, MP, PR, PW, VI)'\n",
    "}\n",
    "\n",
    "DISTANCEONLY = {\n",
    "\t0:\t'Not distance-education only',\n",
    "\t1:\t'Distance-education only'\n",
    "}\n",
    "    \n",
    "CURROPER = {\n",
    "\t0:\t'Not currently certified as an operating institution',\n",
    "\t1:\t'Currently certified as operating'\n",
    "}\n",
    "    \n",
    "SCHTYPE = { \n",
    "\t1:\t'Public',\n",
    "\t2:\t'Private, Nonprofit',\n",
    "\t3:\t'Private, For-profit'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd3 in position 8: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a5345abe18cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'MERGED'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcollege_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd3 in position 8: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# set files path\n",
    "path = 'data/scorecard'\n",
    "\n",
    "# columns to select from datasets\n",
    "college_columns = [ 'UNITID','INSTNM','INSTURL',\\\n",
    "                    'CITY','STABBR','ZIP',\\\n",
    "                    'LATITUDE','LONGITUDE',\\\n",
    "                    'ST_FIPS', 'REGION','SCHTYPE',\\\n",
    "                    'HIGHDEG','CURROPER'\n",
    "                  ]\n",
    "\n",
    "# columns renamed as user friendly labels for use in database\n",
    "college_cols_renamed = {\n",
    "                        'UNITID'    : 'college_id',\n",
    "                        'INSTNM'    : 'name',\n",
    "                        'INSTURL'   : 'website',\n",
    "                        'SCHTYPE'   : 'schtype',\n",
    "                        'CITY'      : 'city',\n",
    "                        'STABBR'    : 'state',\n",
    "                        'ZIP'       : 'zipcode',\n",
    "                        'ST_FIPS'   : 'state_fips',\n",
    "                        'LATITUDE'  : 'latitude',\n",
    "                        'LONGITUDE' : 'longitude',\n",
    "                        'REGION'    : 'region'\n",
    "                       }\n",
    "# create an empty dataframe for storing the CSV files data\n",
    "college_names_df = pd.DataFrame() \n",
    "\n",
    "# loop through files in current directory\n",
    "for filename in os.listdir(path):\n",
    "    if 'MERGED' in filename:\n",
    "        year = filename[6:][:7]\n",
    "        df = pd.read_csv(os.path.join(path, filename), encoding='utf-8', low_memory=False)\n",
    "        df = df[college_columns]\n",
    "        try:\n",
    "            college_names_df = college_names_df.append(df, ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "# keep only the first 5 characters of the zip code\n",
    "college_names_df['ZIP'] = college_names_df['ZIP'].str[:5]\n",
    "\n",
    "cond1 = (college_names_df['CURROPER'] == 1)           # select currently operating schools\n",
    "cond2 = (college_names_df['HIGHDEG'] > 0)             # select degree granting schools only\n",
    "college_names_df = college_names_df[cond1 & cond2]    # filter data and save as updated dataframe\n",
    "\n",
    "# rename columns\n",
    "college_names_df = college_names_df.rename(columns=college_cols_renamed)\n",
    "\n",
    "# drop last two rows\n",
    "df_cols = len(college_names_df.columns)\n",
    "college_names_df.drop(college_names_df.iloc[:, df_cols-2:df_cols], inplace = True, axis = 1)\n",
    "\n",
    "# update region names using the data dictionary provided by College Scorecard\n",
    "college_names_df['region'] = college_names_df['region'].replace(REGION)\n",
    "college_names_df['schtype'] = college_names_df['schtype'].replace(SCHTYPE)\n",
    "\n",
    "# Import schools from Kaggle CSV\n",
    "schools_df = pd.read_csv('data/kaggle/salaries-by-region-id.csv', encoding='utf-8').iloc[:, 0:2]\n",
    "\n",
    "# drop any rows with a missing college id\n",
    "schools_df = schools_df[schools_df['UNITID'].isna() == False]\n",
    "\n",
    "# convert the college id to an integer\n",
    "schools_df = schools_df.astype({ 'UNITID': int })\n",
    "\n",
    "# rename the `UNITID` column to `college_id`\n",
    "schools_df.rename(columns={'UNITID': 'college_id', 'School Name': 'name'}, inplace=True)\n",
    "\n",
    "# drop duplicates\n",
    "schools_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# sort `college_id`\n",
    "schools_df.sort_values(by='college_id', inplace=True)\n",
    "\n",
    "# reset index\n",
    "schools_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Import school types from Kaggle CSV file\n",
    "school_types_df = pd.read_csv('data/kaggle/salaries-by-college-type-id.csv', encoding='utf-8').iloc[:, 0:3]\n",
    "\n",
    "# drop any rows with a missing college id\n",
    "school_types_df = school_types_df[school_types_df['UNITID'].isna() == False]\n",
    "\n",
    "# convert the college id to an integer\n",
    "school_types_df = school_types_df.astype({ 'UNITID': int })\n",
    "\n",
    "# rename the `UNITID` column to `college_id`\n",
    "school_types_df.rename(columns={'UNITID': 'college_id', 'School Name': 'name', 'School Type': 'type'}, inplace=True)\n",
    "\n",
    "# update school types of `Party` or `State` to `Public`\n",
    "school_types_df.loc[school_types_df['type'].isin(['Party','State']), 'type'] = 'Public'\n",
    "\n",
    "# drop duplicates\n",
    "school_types_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# remove bad data\n",
    "school_types_df = school_types_df.drop(school_types_df[(school_types_df['college_id']==233295) & (school_types_df['type']=='Public')].index)\n",
    "\n",
    "# sort by `college_id`\n",
    "school_types_df.sort_values(by='college_id', inplace=True)\n",
    "\n",
    "# reset index\n",
    "school_types_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# merge Kaggle schools data with Scorecard schools data\n",
    "college_names_tbl = pd.merge(schools_df.merge(school_types_df, on=['college_id','name'], how='left'),\n",
    "              college_names_df, on=['college_id'], how='left')\n",
    "\n",
    "# replace missing Kaggle school type with Scorecard school type\n",
    "college_names_tbl.type.fillna(college_names_tbl.schtype, inplace=True)\n",
    "\n",
    "# delete unneeded columns\n",
    "del college_names_tbl['schtype']\n",
    "del college_names_tbl['name_y']\n",
    "\n",
    "# rename `name` column\n",
    "college_names_tbl.rename(columns={'name_x': 'name'}, inplace=True)\n",
    "\n",
    "# set `college_id` as the index\n",
    "college_names_tbl.set_index('college_id', inplace=True)\n",
    "\n",
    "# store table in database\n",
    "college_names_tbl.to_sql(name='college_names', con=engine, if_exists='replace', index=True, method='multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set files path\n",
    "path = 'data/cbsa'\n",
    "\n",
    "# create dataframe to hold all zip code data from files\n",
    "zips_df = pd.DataFrame(columns=['zipcode', 'cbsa_code'])\n",
    "\n",
    "# loop through current working directory ZIP code excel files\n",
    "for filename in os.listdir(path):\n",
    "    if filename.startswith('ZIP')&filename.endswith('.xlsx'):\n",
    "        df = pd.read_excel(os.path.join(path, filename), sheet_name=0, converters={'ZIP': str, 'zip': str})\n",
    "        df = df.rename(columns={'zip': 'zipcode','cbsa': 'cbsa_code', 'ZIP': 'zipcode', 'CBSA': 'cbsa_code'})[['zipcode', 'cbsa_code']]\n",
    "        zips_df = zips_df.append(df, ignore_index=True)\n",
    "\n",
    "# drop any duplicate rows\n",
    "zips_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# sort table by zip code\n",
    "zips_df.sort_values(by=['zipcode'], inplace=True)\n",
    "\n",
    "# reset the index\n",
    "zips_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# cbsa dataframe column names\n",
    "cbsa_columns = [ \n",
    "                 'cbsa_code', 'cbsa_title',\n",
    "                 'csa_code', 'title', \n",
    "                 'state_name', 'state_fips', \n",
    "                 'county', 'county_code'\n",
    "               ]\n",
    "\n",
    "cbsa_cols_renamed = {\n",
    "                        'CBSA Code': 'cbsa_code', 'CBSA Title': 'cbsa_title',\n",
    "                        'CSA Code': 'csa_code', 'CSA Title': 'title', \n",
    "                        'State Name': 'state_name', 'FIPS State Code': 'state_fips',\n",
    "                        'County/County Equivalent': 'county',\n",
    "                        'FIPS County Code': 'county_code'\n",
    "                    }\n",
    "\n",
    "# create dataframe to hold all cbsa code data from files\n",
    "cbsa_df = pd.DataFrame(columns=cbsa_columns)\n",
    "\n",
    "# loop through current working directory CBSA code excel files\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith('_cbsa.xls'):\n",
    "        # read excel file and delete the last three rows containing text\n",
    "        df = pd.read_excel(os.path.join(path, filename), sheet_name=0, skiprows=2).iloc[:-3]\n",
    "        df = df.rename(columns=cbsa_cols_renamed)[cbsa_columns]\n",
    "        cbsa_df = cbsa_df.append(df, ignore_index=True)\n",
    "\n",
    "\n",
    "# drop any duplicate rows\n",
    "cbsa_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# replace missing csa titles with the cbsa title\n",
    "cbsa_df.title.fillna(cbsa_df.cbsa_title, inplace=True)\n",
    "\n",
    "# sort table by zip code\n",
    "cbsa_df.sort_values(by=['state_fips'], inplace=True)\n",
    "\n",
    "# reset the index\n",
    "cbsa_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# make sure that the codes are integers\n",
    "cbsa_int_cols = ['cbsa_code', 'csa_code', 'state_fips', 'county_code']\n",
    "cbsa_df.fillna({ col: 0 for col in cbsa_int_cols }, inplace=True)\n",
    "cbsa_df = cbsa_df.astype({ col: int for col in cbsa_int_cols })\n",
    "\n",
    "# delete unneeded columns\n",
    "del cbsa_df['cbsa_title']\n",
    "\n",
    "# drop rows with any missing data\n",
    "cbsa_df.dropna(how='any', inplace=True)\n",
    "\n",
    "# restructure columns\n",
    "cbsa_df = cbsa_df[['cbsa_code', 'title','state_name','state_fips']]\n",
    "\n",
    "# Glassdoor Metro Areas and lookup keys\n",
    "areas = {\n",
    "    'Atlanta': 'Atlanta',\n",
    "    'Boston': 'Boston',\n",
    "    'Chicago': 'Chicago',\n",
    "    'Houston': 'Houston',\n",
    "    'Los Angeles': 'Los Angeles',\n",
    "    'New York': 'New York City',\n",
    "    'Philadelphia-Camden': 'Philadelphia',\n",
    "    'San Francisco': 'San Francisco',\n",
    "    'Seattle': 'Seattle',\n",
    "    'DC': 'Washington DC'\n",
    "}\n",
    "\n",
    "# create metro column in table with default string 'N/A'\n",
    "cbsa_df['metro'] = 'N/A'\n",
    "\n",
    "# loop through lookup keys\n",
    "for area,metro in areas.items():\n",
    "    # set glassdoor metro name where lookup key is found in CBSA title\n",
    "    cbsa_df.loc[cbsa_df.title.str.contains(area), 'metro'] = metro\n",
    "    \n",
    "# merge transformations into one table for storing to the database\n",
    "regions_tbl = pd.merge(zips_df, cbsa_df, on=['cbsa_code'], how='left')\n",
    "\n",
    "# drop any duplicate rows\n",
    "regions_tbl.drop_duplicates(inplace=True)\n",
    "\n",
    "# drop rows with any missing data\n",
    "regions_tbl.dropna(how='any', thresh=3, inplace=True)\n",
    "\n",
    "# reset the index\n",
    "regions_tbl.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# make sure that the codes are integers\n",
    "regions_int_cols = ['cbsa_code', 'state_fips']\n",
    "regions_tbl.fillna({ col: 0 for col in regions_int_cols }, inplace=True)\n",
    "regions_tbl = regions_tbl.astype({ col: int for col in regions_int_cols })\n",
    "\n",
    "# set `zipcode` as the index\n",
    "regions_tbl.set_index('zipcode', inplace=True)\n",
    "\n",
    "# store table in database\n",
    "regions_tbl.to_sql(name='regions', con=engine, if_exists='replace', index=True, method='multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import National Salary For Each Major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "degreesTPB_df = pd.read_csv(r\"data/kaggle/degrees-that-pay-back.csv\")\n",
    "#degreesTPB_df.head(5)\n",
    "\n",
    "degreesTPB_df = degreesTPB_df.rename(columns={\"Undergraduate Major\": \"Majors\"})\n",
    "#degreesTPB_df.head(5)\n",
    "\n",
    "#Transform Salary to Integers\n",
    "\n",
    "for col in degreesTPB_df.columns:\n",
    "    if 'Salary' in col:\n",
    "        degreesTPB_df[col] = degreesTPB_df[col].replace( '[\\$,)]','', regex=True ).astype(float)\n",
    "        \n",
    "#Drop Unnecessary Columns\n",
    "degreesTPB_df = degreesTPB_df.drop(columns=['Percent change from Starting to Mid-Career Salary', 'Mid-Career 10th Percentile Salary', 'Mid-Career 25th Percentile Salary', 'Mid-Career 75th Percentile Salary', 'Mid-Career 90th Percentile Salary'])\n",
    "degreesTPB_df.head(5)\n",
    "\n",
    "#Connect to local database\n",
    "rds_connection_string = \"postgres:postgres@localhost:5432/HigherEducation\"\n",
    "engine = create_engine(f'postgresql://{rds_connection_string}')\n",
    "\n",
    "#Check For Tables\n",
    "\n",
    "engine.table_names()\n",
    "\n",
    "#Use pandas to load csv converted DataFrame into database\n",
    "\n",
    "if_exists_param = 'replace'\n",
    "\n",
    "degreesTPB_df.to_sql(name='salaries_per_major', con=engine, if_exists=if_exists_param, index=False)\n",
    "#degreesTPB_df['Major_id'].to_sql(name='school_majors', con=engine, if_exists=if_exists_param, index=False)\n",
    "\n",
    "#get table names\n",
    "\n",
    "#inspector = inspect(engine)\n",
    "#inspector.get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
